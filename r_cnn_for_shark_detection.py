# -*- coding: utf-8 -*-
"""R-CNN FOR SHARK DETECTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1llthPsREts3huNvLQxGxSlOQohUASKPL
"""

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="6RW5BXL4z1TBKvr2WJeB")
project = rf.workspace("fyp-b7z9h").project("underwater-species-detection-on-bruvs")
version = project.version(4)
dataset = version.download("coco")

import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from pycocotools.coco import COCO
from PIL import Image
import os
import torchvision


# Define the number of classes including background
num_classes = 4  # 0: background, 1: ray, 2: reef fish, 3: shark

# Custom dataset class for COCO with 4 classes (background, ray, reef fish, shark)
class CustomCocoDataset(torchvision.datasets.CocoDetection):
    def __init__(self, root, annFile, transform=None):
        super(CustomCocoDataset, self).__init__(root, annFile, transform)
        self.coco = COCO(annFile)
        self.root = root
        self.transform = transform

    def __getitem__(self, idx):
        img, target = super(CustomCocoDataset, self).__getitem__(idx)
        image_id = self.ids[idx]
        coco_target = self.coco.loadAnns(self.coco.getAnnIds(imgIds=image_id))

        # Convert coco_target into the format expected by the model
        boxes = []
        labels = []
        for obj in coco_target:
            xmin = obj['bbox'][0]
            ymin = obj['bbox'][1]
            xmax = xmin + obj['bbox'][2]
            ymax = ymin + obj['bbox'][3]
            boxes.append([xmin, ymin, xmax, ymax])

            # Adjust the label according to our classes: background (0), ray (1), reef fish (2), shark (3)
            labels.append(obj['category_id'])  # Assume category_id corresponds directly to the labels

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # Define the target
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels

        # If img is already a tensor, we skip transformation
        if isinstance(img, Image.Image):  # Check if it's a PIL Image
            if self.transform is not None:
                img = self.transform(img)

        return img, target

# Transforms for the images
transform = transforms.Compose([
    transforms.ToTensor()
])

# Load the dataset with 4 classes
dataset = CustomCocoDataset(root='/content/Underwater-Species-Detection-on-BRUVs-4/train', annFile='/content/Underwater-Species-Detection-on-BRUVs-4/train/_annotations.coco.json', transform=transform)
data_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))

# Model initialization
import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator

# Load a pre-trained model for classification and return only the features
backbone = torchvision.models.mobilenet_v2(weights='IMAGENET1K_V1').features
backbone.out_channels = 1280  # Set the output channels for backbone

# Define the anchor generator for the FPN
rpn_anchor_generator = AnchorGenerator(
    sizes=((32, 64, 128, 256, 512),),
    aspect_ratios=((0.5, 1.0, 2.0),) * 5
)

# RoI pooler
roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)

# Put the pieces together inside a Faster RCNN model
model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=rpn_anchor_generator, box_roi_pool=roi_pooler)

# Move model to the appropriate device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Training loop
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)

for epoch in range(10):
    model.train()
    i = 0
    for images, targets in data_loader:
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Skip images without bounding boxes (empty annotations)
        valid_targets = []
        valid_images = []
        for img, target in zip(images, targets):
            if target['boxes'].numel() > 0:  # Check if there are any valid boxes
                valid_images.append(img)
                valid_targets.append(target)

        if len(valid_images) == 0:
            continue  # Skip if no valid images for this batch

        # Sanity check on the labels
        for target in valid_targets:
            if target['labels'].max() >= num_classes:
                raise ValueError(f"Target label out of bounds. Found label {target['labels'].max()} but max class index is {num_classes - 1}.")

        # Forward pass
        loss_dict = model(valid_images, valid_targets)
        losses = sum(loss for loss in loss_dict.values())

        # Backward pass
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        print(f"Epoch {epoch}, Loss: {losses.item()}")

print("Training finished.")

torch.save(model.state_dict(), 'faster_rcnn_underwater_species.pth')

model.eval()
with torch.no_grad():
    for images, _ in data_loader:  # Use a different dataset if you have a test set
        images = list(image.to(device) for image in images)
        outputs = model(images)

        # Visualize the outputs
        for i, output in enumerate(outputs):
            print(f"Image {i} detections:", output['boxes'], output['labels'], output['scores'])

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np

# Function to convert image tensor to numpy for visualization
def visualize_image_with_boxes(image_tensor, boxes, labels, scores, label_map, threshold=0.5):
    # Convert the image tensor to a numpy array
    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()

    # Create a figure and axis
    fig, ax = plt.subplots(1)
    ax.imshow(image_np)

    # Loop over the boxes, labels, and scores
    for box, label, score in zip(boxes, labels, scores):
        if score > threshold:  # Only show boxes above the confidence threshold
            xmin, ymin, xmax, ymax = box.cpu().detach().numpy()
            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')
            ax.add_patch(rect)
            # Add label and score text
            plt.text(xmin, ymin, f'{label_map[label.item()]}: {score:.2f}', color='yellow', fontsize=12)

    # Show the plot
    plt.show()

# Label map for your classes (assuming 0: background, 1: ray, 2: reef fish, 3: shark)
label_map = {0: 'background', 1: 'ray', 2: 'reef fish', 3: 'shark'}

# Visualize the first image in the batch
image = images[0]  # Assuming images is your batch of images
boxes = outputs[0]['boxes']
labels = outputs[0]['labels']
scores = outputs[0]['scores']

# Call the visualization function
visualize_image_with_boxes(image, boxes, labels, scores, label_map)

# Load the test dataset with 4 classes
test_dataset = CustomCocoDataset(root='/content/Underwater-Species-Detection-on-BRUVs-4/test',
                                 annFile='/content/Underwater-Species-Detection-on-BRUVs-4/test/_annotations.coco.json',
                                 transform=transform)
test_data_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))

# Set the model to evaluation mode
model.eval()

# No need to calculate gradients during inference
with torch.no_grad():
    for images, targets in test_data_loader:
        images = list(image.to(device) for image in images)

        # Get the predictions (outputs)
        outputs = model(images)

        # Visualize results for a few test images
        num_images = len(images)
        num_samples = min(3, num_images)  # Display up to 3 images

        for idx in range(num_samples):
            image = images[idx]
            boxes = outputs[idx]['boxes']
            labels = outputs[idx]['labels']
            scores = outputs[idx]['scores']

            # Call the visualization function to display results
            visualize_image_with_boxes(image, boxes, labels, scores, label_map)

import torch

# Function to calculate IoU between two bounding boxes
def calculate_iou(box1, box2):
    # box1 and box2 are arrays or tensors of form [xmin, ymin, xmax, ymax]
    xmin1, ymin1, xmax1, ymax1 = box1
    xmin2, ymin2, xmax2, ymax2 = box2

    # Calculate the intersection coordinates
    inter_xmin = max(xmin1, xmin2)
    inter_ymin = max(ymin1, ymin2)
    inter_xmax = min(xmax1, xmax2)
    inter_ymax = min(ymax1, ymax2)

    # Calculate the area of the intersection
    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)

    # Calculate the area of both boxes
    box1_area = (xmax1 - xmin1) * (ymax1 - ymin1)
    box2_area = (xmax2 - xmin2) * (ymax2 - ymin2)

    # Calculate the union area
    union_area = box1_area + box2_area - inter_area

    # Return the IoU
    iou = inter_area / union_area if union_area > 0 else 0
    return iou

# Evaluate the model on the test dataset and calculate performance metrics
def evaluate_model(model, test_data_loader, iou_threshold=0.5):
    model.eval()  # Set the model to evaluation mode
    all_ious = []  # Store IoUs to compute mean IoU later
    correct_predictions = 0
    total_predictions = 0
    total_ground_truths = 0

    with torch.no_grad():
        for images, targets in test_data_loader:
            images = list(image.to(device) for image in images)
            outputs = model(images)

            # Iterate over images and their predictions
            for i, output in enumerate(outputs):
                pred_boxes = output['boxes'].cpu()
                pred_labels = output['labels'].cpu()
                pred_scores = output['scores'].cpu()

                gt_boxes = targets[i]['boxes'].cpu()
                gt_labels = targets[i]['labels'].cpu()

                total_ground_truths += len(gt_boxes)
                total_predictions += len(pred_boxes)

                # Compare predicted boxes with ground truth boxes
                for gt_box in gt_boxes:
                    best_iou = 0
                    for pred_box, pred_label in zip(pred_boxes, pred_labels):
                        iou = calculate_iou(gt_box, pred_box)
                        best_iou = max(best_iou, iou)
                    all_ious.append(best_iou)
                    if best_iou > iou_threshold:
                        correct_predictions += 1

    mean_iou = sum(all_ious) / len(all_ious) if len(all_ious) > 0 else 0
    precision = correct_predictions / total_predictions if total_predictions > 0 else 0
    recall = correct_predictions / total_ground_truths if total_ground_truths > 0 else 0

    return {
        'mean_iou': mean_iou,
        'precision': precision,
        'recall': recall,
        'total_predictions': total_predictions,
        'total_ground_truths': total_ground_truths
    }

# Calculate and print the performance metrics
performance_metrics = evaluate_model(model, test_data_loader, iou_threshold=0.5)
print(f"Mean IoU: {performance_metrics['mean_iou']:.4f}")
print(f"Precision: {performance_metrics['precision']:.4f}")
print(f"Recall: {performance_metrics['recall']:.4f}")
print(f"Total Predictions: {performance_metrics['total_predictions']}")
print(f"Total Ground Truths: {performance_metrics['total_ground_truths']}")

# Evaluate the model on the test dataset and calculate performance metrics
def evaluate_model(model, test_data_loader, iou_threshold=0.5):
    # Load the saved model
    model = FasterRCNN(backbone, num_classes=num_classes, rpn_anchor_generator=rpn_anchor_generator, box_roi_pool=roi_pooler)
    model.load_state_dict(torch.load('/content/faster_rcnn_underwater_species.pth', weights_only=True))  # Set weights_only=True
    model.to(device)  # Move the model to the appropriate device
    model.eval()  # Set the model to evaluation mode
    all_ious = []  # Store IoUs to compute mean IoU later
    correct_predictions = 0
    total_predictions = 0
    total_ground_truths = 0

    # Initialize counters for each class
    class_correct = {0: 0, 1: 0, 2: 0, 3: 0}  # For background, ray, reef fish, shark
    class_total = {0: 0, 1: 0, 2: 0, 3: 0}  # For background, ray, reef fish, shark

    with torch.no_grad():
        for images, targets in test_data_loader:
            images = list(image.to(device) for image in images)
            outputs = model(images)

            # Iterate over images and their predictions
            for i, output in enumerate(outputs):
                pred_boxes = output['boxes'].cpu()
                pred_labels = output['labels'].cpu()
                pred_scores = output['scores'].cpu()

                gt_boxes = targets[i]['boxes'].cpu()
                gt_labels = targets[i]['labels'].cpu()

                total_ground_truths += len(gt_boxes)
                total_predictions += len(pred_boxes)

                # Compare predicted boxes with ground truth boxes
                for gt_box, gt_label in zip(gt_boxes, gt_labels):
                    best_iou = 0
                    best_pred_label = -1

                    for pred_box, pred_label in zip(pred_boxes, pred_labels):
                        iou = calculate_iou(gt_box, pred_box)
                        if iou > best_iou:
                            best_iou = iou
                            best_pred_label = pred_label

                    all_ious.append(best_iou)
                    if best_iou > iou_threshold:
                        correct_predictions += 1
                        class_correct[best_pred_label.item()] += 1
                    class_total[gt_label.item()] += 1  # Increment the total for the ground truth class

    mean_iou = sum(all_ious) / len(all_ious) if len(all_ious) > 0 else 0
    precision = correct_predictions / total_predictions if total_predictions > 0 else 0
    recall = correct_predictions / total_ground_truths if total_ground_truths > 0 else 0

    # Calculate precision for each class
    precision_per_class = {}
    for class_id in range(len(class_correct)):
        if class_total[class_id] > 0:
            precision_per_class[class_id] = class_correct[class_id] / class_total[class_id]
        else:
            precision_per_class[class_id] = 0.0  # If there are no ground truth boxes for this class

    return {
        'mean_iou': mean_iou,
        'precision': precision,
        'recall': recall,
        'total_predictions': total_predictions,
        'total_ground_truths': total_ground_truths,
        'precision_per_class': precision_per_class  # Return precision for each class
    }

# Calculate and print the performance metrics
performance_metrics = evaluate_model(model, test_data_loader, iou_threshold=0.5)
print(f"Mean IoU: {performance_metrics['mean_iou']:.4f}")
print(f"Precision (Overall): {performance_metrics['precision']:.4f}")
print(f"Recall: {performance_metrics['recall']:.4f}")
print(f"Total Predictions: {performance_metrics['total_predictions']}")
print(f"Total Ground Truths: {performance_metrics['total_ground_truths']}")

# Print precision for each class
for class_id, precision in performance_metrics['precision_per_class'].items():
    if class_id ==3:
      print(f"Precision for class sharks: {precision:.4f}")
    print(f"Precision for class {class_id}: {precision:.4f}")